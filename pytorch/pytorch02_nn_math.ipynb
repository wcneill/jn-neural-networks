{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at the Math Behind a Neural Network:\n",
    "A primer on the basic linear algebra that goes on under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    \"\"\"\n",
    "    sigmoid activation function on x\n",
    "    \"\"\"\n",
    "    return 1/(1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single layer network:\n",
    "torch.manual_seed(7)\n",
    "features = torch.randn((1,5))\n",
    "weights = torch.randn_like(features)\n",
    "bias = torch.randn((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1595]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = activation(torch.mm(features, weights.T) + bias)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3171]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two layer network (not including input layer): \n",
    "torch.manual_seed(7)\n",
    "\n",
    "features = torch.randn((1,3))\n",
    "\n",
    "# define network architecture:\n",
    "n_input = features.shape[1]\n",
    "n_hidden = 2\n",
    "n_output = 1\n",
    "\n",
    "# init weight and bias matrices:\n",
    "W1 = torch.randn((n_input, n_hidden))\n",
    "W2 = torch.randn((n_hidden, n_output))\n",
    "b1 = torch.randn((1, n_hidden))\n",
    "b2 = torch.randn((1, n_output))\n",
    "\n",
    "# feed forward:\n",
    "a1 = activation(torch.mm(features, W1) + b1)\n",
    "output = activation(torch.mm(a1, W2) + b2)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Generate some data\n",
    "torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "\n",
    "# Features are 3 random normal variables\n",
    "features = torch.randn((1, 3))\n",
    "\n",
    "# Define the size of each layer in our network\n",
    "n_input = features.shape[1]     # Number of input units, must match number of input features\n",
    "n_hidden = 2                    # Number of hidden units \n",
    "n_output = 1                    # Number of output units\n",
    "\n",
    "# Weights for inputs to hidden layer\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "# Weights for hidden layer to output layer\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# and bias terms for hidden and output layers\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "B2 = torch.randn((1, n_output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
